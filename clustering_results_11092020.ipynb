{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accelerated agglomerative hierachical clustering with multicomparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of agglomerative hierachical clustering: \n",
    "- n samples from molecular dynamics simulation.\n",
    "- convert each frame into contact fingerprint then forming input with size (n,m), where n is sample number and m is the length of each fingerprint.\n",
    "- initialization of binary similarity matrix by comparing each two fingerprints then forming output binary similarity matrix with size (n,n) and upper diagonal 0. __e.g.__ result from 1000 fingerprints binary comparison with the similarity defined by sum(a,d), where a is the number of coincident 1's and b is the number of coincident 0's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "       [724.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "       [714., 702.,   0., ...,   0.,   0.,   0.],\n",
       "       ...,\n",
       "       [680., 684., 666., ...,   0.,   0.,   0.],\n",
       "       [684., 680., 674., ..., 706.,   0.,   0.],\n",
       "       [684., 680., 680., ..., 698., 734.,   0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load('test_1000samples_simi_matrix.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initialization binary similarity matrix speed test --- complexity $O(n^2)$, pretty fast with multi-binary comparison implementation (can be ignored compared with cost in following clustering step)\n",
    "\n",
    "    ![](test_initialization_speed.png)\n",
    "\n",
    "- start agglomerative hierachical clustering by updating the above like binary similarity matrix. At each clustering step, calculating the similarity of group any two clusters.\n",
    "\n",
    "  The size of cluster will become larger along clustering process, just like the words from Ramon's note, if we have two sets, A and B, each with Na and Nb elements, respectively, this will scale as $O(NaNb) \\approx O(N^2)$, which leads current algorithm scaled as $O(N^3)$ in total. However, if we link the sets by considering which of them will be more similar after we combine them, and we use the multiple comparisons described above, this will scale as $O(Na + Nb) \\approx O(N)$, which leads our implementation scaled as $O(N^2)$ in total. I believe this can be further improved by implemented with data structure like heap with some scrifice of memory, then the number of clustering steps N will scale to $logN$ and the overall complexity will be $O(NlogN)$\n",
    "\n",
    "  I found this paper seems to be the current fastest implementation. https://www.jstatsoft.org/article/view/v053i09\n",
    "  \n",
    "  \n",
    "- Clustering step speed test --- complexity $O(N^2)$\n",
    "\n",
    "  Note: the time is the average time of 10 runs with green error bar as 2*std\n",
    "\n",
    "    ![](test_clustering_speed.png)\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering quality test\n",
    "\n",
    "- case 1: 1000 protein samples with rmsd compared to native structure.\n",
    "\n",
    "    ![](test_proteinG_sample_rmsd.png)\n",
    "\n",
    "  __e.g.__: clustering results of largest ~10 clusters from the 975th step.\n",
    "  \n",
    "|1d rmsd; side bar shows number of samples / 1000 in each cluster |2d rmsd; blue square block shows ~10 clusters intra-rmsd, good\n",
    "    |- | - |\n",
    "|    ![alt](test_1000samples_-25cluster_1drmsd.png) | ![alt](test_1000samples_-25cluster_2drmsd.png )|\n",
    "\n",
    "---\n",
    "\n",
    "- case 2: 1000 protein-DNA binding samples with rmsd compared to native structure.\n",
    "\n",
    "    ![](test_binding_sample_rmsd.png)\n",
    "    \n",
    "  __e.g.__: clustering results of largest ~10 clusters from the 975th step. (Note: the fingerprint is constructed on the binding interface and the following rmsd is calculated over all \"C\" atoms in the system, so each blue block is not \"that blue\" compared with case 1, but still identifiable.)\n",
    "  \n",
    "|1d rmsd; side bar shows number of samples / 1000 in each cluster |2d rmsd; blue square block shows ~10 clusters intra-rmsd, good\n",
    "    |- | - |\n",
    "|    ![alt](test_binding_1000samples_-22cluster_1drmsd.png) | ![alt](test_binding_1000samples_-22cluster_2drmsd.png)|  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering accuracy for all clustering step\n",
    "\n",
    "   ![](test_proteinG_1000samples_accuracy.png)\n",
    "\n",
    "---\n",
    "\n",
    "- This simply implies that as more samples goes into one cluster, the cluster will become less clean. But overall, it's almost maintain ~ 90%.\n",
    "\n",
    "- __Compared with Faith index__\n",
    "\n",
    "  This implies that Faith index tends to group more samples with less similarity into one cluster in ealier clustering steps.\n",
    "\n",
    "|accuracy vs clustering step |2d rmsd; blue square block shows ~5 clusters intra-rmsd, not good\n",
    "    |- | - |\n",
    "|    ![alt](test_Faith_proteinG_1000samples_accuracy.png) | ![alt](test_Faith_1000samples_-40cluster_2drmsd.png)|\n",
    "\n",
    "    \n",
    "---"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
